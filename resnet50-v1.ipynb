{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30302,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**import dataset**","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/elaamranisoufiane/Comp2018DatasetCleanISR.git","metadata":{"execution":{"iopub.status.busy":"2024-06-10T13:44:42.775865Z","iopub.execute_input":"2024-06-10T13:44:42.776822Z","iopub.status.idle":"2024-06-10T13:44:47.246951Z","shell.execute_reply.started":"2024-06-10T13:44:42.776727Z","shell.execute_reply":"2024-06-10T13:44:47.245915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# import tensorflow as tf\n# from tensorflow.keras import backend as K\n\n# from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n# from tensorflow.keras.models import Model\n# from matplotlib import pyplot as plt\n# import matplotlib.image as mpimg\n\n# def iou(y_true, y_pred):\n#     def f(y_true, y_pred):\n#         intersection = (y_true * y_pred).sum()\n#         union = y_true.sum() + y_pred.sum() - intersection\n#         x = (intersection + 1e-15) / (union + 1e-15)\n#         x = x.astype(np.float32)\n#         return x\n#     return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n\n# smooth = 1e-15\n# def dice_coef(y_true, y_pred):\n#     y_true = tf.keras.layers.Flatten()(y_true)\n#     y_pred = tf.keras.layers.Flatten()(y_pred)\n#     intersection = tf.reduce_sum(y_true * y_pred)\n#     return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n\n# def dice_loss(y_true, y_pred):\n#     return 1.0 - dice_coef(y_true, y_pred)\n\n# from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Conv2DTranspose, Concatenate, Input\n# from tensorflow.keras.models import Model\n# from tensorflow.keras.applications import ResNet50\n\n# def conv_block(inputs, num_filters):\n#     x = Conv2D(num_filters, 3, padding=\"same\")(inputs)\n#     x = BatchNormalization()(x)\n#     x = Activation(\"relu\")(x)\n\n#     x = Conv2D(num_filters, 3, padding=\"same\")(x)\n#     x = BatchNormalization()(x)\n#     x = Activation(\"relu\")(x)\n\n#     return x\n\n# def decoder_block(inputs, skip_features, num_filters):\n#     x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs)\n#     x = Concatenate()([x, skip_features])\n#     x = conv_block(x, num_filters)\n#     return x\n\n# def build_resnet50_unet(input_shape):\n#     \"\"\" Input layer \"\"\"\n#     inputs = Input(input_shape)\n\n#     \"\"\" Pre-trained ResNet50 Model \"\"\"\n#     resnet50 = ResNet50(include_top=False, weights='imagenet', input_tensor=inputs)\n    \n#     # Extract the feature maps\n#     s1 = resnet50.get_layer(\"conv1_relu\").output           # 64\n#     s2 = resnet50.get_layer(\"conv2_block3_out\").output     # 256\n#     s3 = resnet50.get_layer(\"conv3_block4_out\").output     # 512\n#     s4 = resnet50.get_layer(\"conv4_block6_out\").output     # 1024\n#     b1 = resnet50.get_layer(\"conv5_block3_out\").output     # 2048\n\n#     \"\"\" Decoder \"\"\"\n#     d1 = decoder_block(b1, s4, 512)\n#     d2 = decoder_block(d1, s3, 256)\n#     d3 = decoder_block(d2, s2, 128)\n#     d4 = decoder_block(d3, s1, 64)\n#     d5 = Conv2DTranspose(32, (2, 2), strides=2, padding=\"same\")(d4)\n#     d6 = conv_block(d5, 16)\n\n#     \"\"\" Output layer \"\"\"\n#     outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d6)\n\n#     model = Model(inputs, outputs, name=\"ResNet50_UNET\")\n#     return model\n\n# if __name__ == \"__main__\":\n#     model = build_resnet50_unet((1024, 1024, 3))\n#     model.summary()\n\n ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **##test##**","metadata":{}},{"cell_type":"code","source":"import os\n\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\nimport numpy as np\nimport cv2\nfrom glob import glob\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import Recall, Precision\nfrom sklearn.model_selection import KFold\nfrom tqdm import tqdm\nimport pandas as pd\n\n#import build_unet\n#import dice_coef, iou\n \nfrom tensorflow.keras.utils import CustomObjectScope\nfrom sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n\n\n# input image size\nH = 1024\nW = 1024\n\ndef iou(y_true, y_pred):\n    def f(y_true, y_pred):\n        intersection = (y_true * y_pred).sum()\n        union = y_true.sum() + y_pred.sum() - intersection\n        x = (intersection + 1e-15) / (union + 1e-15)\n        x = x.astype(np.float32)\n        return x\n    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n\nsmooth = 1e-15\ndef dice_coef(y_true, y_pred):\n    y_true = tf.keras.layers.Flatten()(y_true)\n    y_pred = tf.keras.layers.Flatten()(y_pred)\n    intersection = tf.reduce_sum(y_true * y_pred)\n    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n\ndef dice_loss(y_true, y_pred):\n    return 1.0 - dice_coef(y_true, y_pred)\n\n# create directory and gives the path\ndef create_dir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\ndef shuffling(x, y):\n    x, y = shuffle(x, y, random_state=42)\n\n\ndef read_image(path):\n    path = path.decode()\n    x = cv2.imread(path, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (W, H))\n    x = x / 255.0\n    x = x.astype(np.float32)\n    return x\n\n\ndef read_mask(path):\n    path = path.decode()\n    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, (W, H))\n    x = x / 255.0\n    x = x.astype(np.float32)\n    x = np.expand_dims(x, axis=-1)\n    return x\n\n\ndef tf_parse(x, y):\n    def _parse(x, y):\n        x = read_image(x)\n        y = read_mask(y)\n        return x, y\n\n    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n    x.set_shape([H, W, 3])\n    y.set_shape([H, W, 1])\n    return x, y\n\n\n\ndef tf_dataset(X, Y, batch=8):\n    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n    dataset = dataset.map(tf_parse)\n    dataset = dataset.batch(batch)\n    dataset = dataset.prefetch(10)\n    return dataset\n\n##########################################\n\ndef create_dir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\ndef read_image1(path):\n    x = cv2.imread(path, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (W, H))\n    ori_x = x\n    x = x / 255.0\n    x = x.astype(np.float32)\n    x = np.expand_dims(x, axis=0)  ## (1, 256, 256, 3)\n    return ori_x, x\n\n\ndef read_mask1(path):\n    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, (W, H))\n    ori_x = x\n    x = x / 255.0\n    x = x > 0.5\n    x = x.astype(np.int32)\n    return ori_x, x\n\ndef save_result(ori_x, ori_y, y_pred, save_path):\n    line = np.ones((H, 10, 3)) * 255\n\n    ori_y = np.expand_dims(ori_y, axis=-1)  ## (256, 256, 1)\n    ori_y = np.concatenate([ori_y, ori_y, ori_y], axis=-1)  ## (256, 256, 3)\n\n    y_pred = np.expand_dims(y_pred, axis=-1)\n    y_pred = np.concatenate([y_pred, y_pred, y_pred], axis=-1) * 255.0\n\n    cat_images = np.concatenate([ori_x, line, ori_y, line, y_pred], axis=1)\n    cv2.imwrite(save_path, cat_images)\n\ndef load_data(path, split=0.2):\n    images = sorted(glob(os.path.join(path, \"images\", \"*.jpg\")))\n    masks = sorted(glob(os.path.join(path, \"Masks\", \"*.jpg\")))\n    size = int(len(images) * split)\n\n    train_x, valid_x = train_test_split(images, test_size=size, random_state=42)\n    train_y, valid_y = train_test_split(masks, test_size=size, random_state=42)\n\n    train_x, test_x = train_test_split(train_x, test_size=size, random_state=42)\n    train_y, test_y = train_test_split(train_y, test_size=size, random_state=42) \n    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T13:46:09.632480Z","iopub.execute_input":"2024-06-10T13:46:09.632881Z","iopub.status.idle":"2024-06-10T13:46:09.666118Z","shell.execute_reply.started":"2024-06-10T13:46:09.632846Z","shell.execute_reply":"2024-06-10T13:46:09.665122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Conv2DTranspose, Concatenate, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import Recall, Precision\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, f1_score, jaccard_score, recall_score, precision_score\nfrom tqdm import tqdm\nimport pandas as pd\nimport os\n\n# Ensure the required functions such as `create_dir`, `load_data`, `tf_dataset`, `read_image1`, `read_mask1`, and `save_result` are defined here or imported\n\ndef conv_block(inputs, num_filters):\n    x = Conv2D(num_filters, 3, padding=\"same\")(inputs)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    return x\n\ndef decoder_block(inputs, skip_features, num_filters):\n    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs)\n    x = Concatenate()([x, skip_features])\n    x = conv_block(x, num_filters)\n    return x\n\ndef build_resnet50_unet(input_shape):\n    \"\"\" Input layer \"\"\"\n    inputs = Input(input_shape)\n\n    \"\"\" Pre-trained ResNet50 Model \"\"\"\n    resnet50 = ResNet50(include_top=False, weights='imagenet', input_tensor=inputs)\n    \n    # Extract the feature maps\n    s1 = resnet50.get_layer(\"conv1_relu\").output           # 64\n    s2 = resnet50.get_layer(\"conv2_block3_out\").output     # 256\n    s3 = resnet50.get_layer(\"conv3_block4_out\").output     # 512\n    s4 = resnet50.get_layer(\"conv4_block6_out\").output     # 1024\n    b1 = resnet50.get_layer(\"conv5_block3_out\").output     # 2048\n\n    \"\"\" Decoder \"\"\"\n    d1 = decoder_block(b1, s4, 512)\n    d2 = decoder_block(d1, s3, 256)\n    d3 = decoder_block(d2, s2, 128)\n    d4 = decoder_block(d3, s1, 64)\n    d5 = Conv2DTranspose(32, (2, 2), strides=2, padding=\"same\")(d4)\n    d6 = conv_block(d5, 16)\n\n    \"\"\" Output layer \"\"\"\n    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d6)\n\n    model = Model(inputs, outputs, name=\"ResNet50_UNET\")\n    return model\n\nif __name__ == \"__main__\":\n    \"\"\" Seeding \"\"\"\n    np.random.seed(42)\n    tf.random.set_seed(42)\n\n    \"\"\" Create directories \"\"\"\n    def create_dir(path):\n        if not os.path.exists(path):\n            os.makedirs(path)\n            \n    create_dir('files')\n    create_dir(\"results\")\n\n    \"\"\" Hyperparameters \"\"\"\n    batch_size = 4\n    lr = 1e-4  ## 0.0001\n    num_epochs = 40\n    model_path = \"files/model.h5\"\n    csv_path = \"files/data.csv\"\n    dataset_path = \"./Comp2018DatasetCleanISR\"\n    \n    \"\"\" Dataset \"\"\"\n    (train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(dataset_path)\n    \n    print(f\"Train: {len(train_x)} - {len(train_y)}\")\n    print(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\n    print(f\"Test: {len(test_x)} - {len(test_y)}\")\n    \n    acc_per_fold = []\n    loss_per_fold = []\n    f1_value_per_fold = []\n    jac_value_per_fold = []\n    recall_value_per_fold = []\n    precision_value_per_fold = []\n    \n    num_folds = 4\n    # Define the K-fold Cross Validator\n    kfold = KFold(n_splits=num_folds, shuffle=True)\n    \n    # K-fold Cross Validation model evaluation\n    fold_no = 1\n    for train, test in kfold.split(train_x, train_y):\n        print(f'Training for fold {fold_no} ...')\n        \n        train_x1 = [train_x[i] for i in train]\n        train_y1 = [train_y[i] for i in train]\n        test_x1 = [train_x[i] for i in test]\n        test_y1 = [train_y[i] for i in test]\n\n        print('------------------------------------------------------------------------')\n        print(f'Training for fold {fold_no} ...')\n        print(f\"Train: {len(train_x1)} - {len(train_y1)}\")\n        print(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\n        print(f\"Test: {len(test_x1)} - {len(test_y1)}\")\n        \n        train_dataset = tf_dataset(train_x1, train_y1, batch=batch_size)\n        valid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)\n        train_steps = (len(train_x1) // batch_size)\n        valid_steps = (len(valid_x) // batch_size)\n        if len(train_x1) % batch_size != 0:\n            train_steps += 1\n\n        for x, y in valid_dataset:\n            print(x.shape, y.shape)\n            break\n\n        \"\"\" Build the model with ResNet50 U-Net network architecture \"\"\"\n        model = build_resnet50_unet((1024, 1024, 3))\n        metrics = [\"accuracy\", dice_coef, iou, Recall(), Precision()]\n        model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr), metrics=metrics)\n        \n        callbacks = [\n            ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n            ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n            CSVLogger(csv_path),\n        ]\n        \n        history = model.fit(\n            train_dataset,\n            epochs=num_epochs,\n            validation_data=valid_dataset,\n            steps_per_epoch=train_steps,\n            validation_steps=valid_steps,\n            callbacks=callbacks,\n            shuffle=False\n        )\n        fold_no = fold_no + 1\n\n        \"\"\" Load Model \"\"\"\n        with CustomObjectScope({'iou': iou, 'dice_coef': dice_coef}):\n            model = tf.keras.models.load_model(model_path)\n        \n        \"\"\" Prediction and metrics values \"\"\"\n        SCORE = []\n        for x, y in tqdm(zip(test_x1, test_y1), total=len(test_x1)):\n            name = x.split(\"/\")[-1]\n            \n            \"\"\" Reading the image and mask \"\"\"\n            ori_x, x = read_image1(x)\n            ori_y, y = read_mask1(y)\n            \n            \"\"\" Prediction \"\"\"\n            y_pred = model.predict(x)[0] > 0.5\n            y_pred = np.squeeze(y_pred, axis=-1)\n            y_pred = y_pred.astype(np.int32)\n            \n            save_path = f\"results/{name}\"\n            save_result(ori_x, ori_y, y_pred, save_path)\n            \n            \"\"\" Flattening the numpy arrays. \"\"\"\n            y = y.flatten()\n            y_pred = y_pred.flatten()\n            \n            \"\"\" Calculating metrics values \"\"\"\n            acc_value = accuracy_score(y, y_pred)\n            f1_value = f1_score(y, y_pred, labels=[0, 1], average=\"binary\")\n            jac_value = jaccard_score(y, y_pred, labels=[0, 1], average=\"binary\")\n            recall_value = recall_score(y, y_pred, labels=[0, 1], average=\"binary\")\n            precision_value = precision_score(y, y_pred, labels=[0, 1], average=\"binary\")\n            SCORE.append([name, acc_value, f1_value, jac_value, recall_value, precision_value])\n\n        \"\"\" Metrics values \"\"\"\n        score = [s[1:] for s in SCORE]\n        score = np.mean(score, axis=0)\n        print(f\"Accuracy: {score[0]:0.5f}\")\n        print(f\"F1: {score[1]:0.5f}\")\n        print(f\"Jaccard: {score[2]:0.5f}\")\n        print(f\"Recall: {score[3]:0.5f}\")\n        print(f\"Precision: {score[4]:0.5f}\")\n        \n        \"\"\" Save the results \"\"\"\n        acc_per_fold.append(score[0])\n        f1_value_per_fold.append(score[1])\n        jac_value_per_fold.append(score[2])\n        recall_value_per_fold.append(score[3])\n        precision_value_per_fold.append(score[4])\n        \n        \"\"\" Saving all the results \"\"\"\n        df = pd.DataFrame(SCORE, columns=[\"Image\", \"Accuracy\", \"F1\", \"Jaccard\", \"Recall\", \"Precision\"])\n        df.to_csv(\"files/score.csv\")\n\n    \"\"\" Final metrics \"\"\"\n    print(f\"Accuracy: {acc_per_fold}\")\n    print(f\"F1: {f1_value_per_fold}\")\n    print(f\"Jaccard: {jac_value_per_fold}\")\n    print(f\"Recall: {recall_value_per_fold}\")\n    print(f\"Precision: {precision_value_per_fold}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T13:46:19.486923Z","iopub.execute_input":"2024-06-10T13:46:19.487376Z","iopub.status.idle":"2024-06-10T17:17:10.769762Z","shell.execute_reply.started":"2024-06-10T13:46:19.487339Z","shell.execute_reply":"2024-06-10T17:17:10.768769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nprint(f\"Accuracy: {np.ceil(np.mean(acc_per_fold)*10000)/100}%\")\nprint(f\"F1: {np.ceil(np.mean(f1_value_per_fold)*10000)/100}%\")\nprint(f\"Jaccard: {np.ceil(np.mean(jac_value_per_fold)*10000)/100}%\")\nprint(f\"Recall: {np.ceil(np.mean(recall_value_per_fold)*10000)/100}%\")\nprint(f\"Precision: {np.ceil(np.mean(precision_value_per_fold)*10000)/100}%\")","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:17:10.771930Z","iopub.execute_input":"2024-06-10T17:17:10.772432Z","iopub.status.idle":"2024-06-10T17:17:10.779360Z","shell.execute_reply.started":"2024-06-10T17:17:10.772391Z","shell.execute_reply":"2024-06-10T17:17:10.778145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, 'y', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.savefig('loss.png')\nplt.show()\n#plt.imshow(mpimg.imread('./loss.png'))\n\naccu = history.history['accuracy']\nval_accu = history.history['val_accuracy']\nepochs = range(1, len(accu) + 1)\nplt.plot(epochs, accu, 'r', label='accuracy') \nplt.plot(epochs, val_accu, 'b', label='val_accuracy') \nplt.title('accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.savefig('accuracy.png')\nplt.show()\n#plt.imshow(mpimg.imread('./accuracy.png'))","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:20:03.199423Z","iopub.execute_input":"2024-06-10T17:20:03.200193Z","iopub.status.idle":"2024-06-10T17:20:03.797131Z","shell.execute_reply.started":"2024-06-10T17:20:03.200132Z","shell.execute_reply":"2024-06-10T17:20:03.796195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Testing**","metadata":{}},{"cell_type":"code","source":"os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\nimport numpy as np\nimport cv2\nimport pandas as pd\nfrom glob import glob\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras.utils import CustomObjectScope\nfrom sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n#import dice_coef, iou\n#import load_data\n\nH = 1024\nW = 1024\n\n\ndef create_dir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\ndef read_image(path):\n    x = cv2.imread(path, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (W, H))\n    ori_x = x\n    x = x / 255.0\n    x = x.astype(np.float32)\n    x = np.expand_dims(x, axis=0)  ## (1, 256, 256, 3)\n    return ori_x, x\n\n\ndef read_mask(path):\n    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, (W, H))\n    ori_x = x\n    x = x / 255.0\n    x = x > 0.5\n    x = x.astype(np.int32)\n    return ori_x, x\n\n\ndef save_result(ori_x, ori_y, y_pred, save_path):\n    line = np.ones((H, 10, 3)) * 255\n\n    ori_y = np.expand_dims(ori_y, axis=-1)  ## (256, 256, 1)\n    ori_y = np.concatenate([ori_y, ori_y, ori_y], axis=-1)  ## (256, 256, 3)\n\n    y_pred = np.expand_dims(y_pred, axis=-1)\n    y_pred = np.concatenate([y_pred, y_pred, y_pred], axis=-1) * 255.0\n\n    cat_images = np.concatenate([ori_x, line, ori_y, line, y_pred], axis=1)\n    cv2.imwrite(save_path, cat_images)\n\n\nif __name__ == \"__main__\":\n    create_dir(\"results\")\n\n    \"\"\" Load Model \"\"\"\n    with CustomObjectScope({'iou': iou, 'dice_coef': dice_coef}):\n        model = tf.keras.models.load_model(\"files/model.h5\")\n        #model.summary()\n\n    \"\"\" Dataset \"\"\"\n    dataset_path = \"./Comp2018DatasetCleanISR\"\n    #dataset_path = \"/imagesToAugment/images\"\n    (train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(dataset_path)\n\n    \"\"\" Prediction and metrics values \"\"\"\n    SCORE = []\n    for x, y in tqdm(zip(test_x, test_y), total=len(test_x)):\n        name = x.split(\"/\")[-1]\n\n        \"\"\" Reading the image and mask \"\"\"\n        ori_x, x = read_image(x)\n        ori_y, y = read_mask(y)\n\n        \"\"\" Prediction \"\"\"\n        y_pred = model.predict(x)[0] > 0.5\n        y_pred = np.squeeze(y_pred, axis=-1)\n        y_pred = y_pred.astype(np.int32)\n\n        save_path = f\"results/{name}\"\n        save_result(ori_x, ori_y, y_pred, save_path)\n\n        \"\"\" Flattening the numpy arrays. \"\"\"\n        y = y.flatten()\n        y_pred = y_pred.flatten()\n\n        \"\"\" Calculating metrics values \"\"\"\n        acc_value = accuracy_score(y, y_pred)\n        f1_value = f1_score(y, y_pred, labels=[0, 1], average=\"binary\")\n        jac_value = jaccard_score(y, y_pred, labels=[0, 1], average=\"binary\")\n        recall_value = recall_score(y, y_pred, labels=[0, 1], average=\"binary\")\n        precision_value = precision_score(y, y_pred, labels=[0, 1], average=\"binary\")\n        SCORE.append([name, acc_value, f1_value, jac_value, recall_value, precision_value])\n\n    \"\"\" Metrics values \"\"\"\n    score = [s[1:] for s in SCORE]\n    score = np.mean(score, axis=0)\n    print(f\"Accuracy: {score[0]:0.5f}\")\n    print(f\"F1: {score[1]:0.5f}\")\n    print(f\"Jaccard: {score[2]:0.5f}\")\n    print(f\"Recall: {score[3]:0.5f}\")\n    print(f\"Precision: {score[4]:0.5f}\")\n\n    \"\"\" Saving all the results \"\"\"\n    df = pd.DataFrame(SCORE, columns=[\"Image\", \"Accuracy\", \"F1\", \"Jaccard\", \"Recall\", \"Precision\"])\n    df.to_csv(\"files/score.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:21:55.575492Z","iopub.execute_input":"2024-06-10T17:21:55.575909Z","iopub.status.idle":"2024-06-10T17:24:37.482243Z","shell.execute_reply.started":"2024-06-10T17:21:55.575876Z","shell.execute_reply":"2024-06-10T17:24:37.481205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport glob\nimport matplotlib.pyplot as plt\n    \npath=\"./results/*.jpg\"\n\nimages=[cv2.imread(image) for image in glob.glob(path)]\nfig=plt.figure()\nfor i in range(len(images)):\n    #plt.subplot(5,5,i+1)\n    plt.imshow(images[i])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:24:37.484665Z","iopub.execute_input":"2024-06-10T17:24:37.485438Z","iopub.status.idle":"2024-06-10T17:29:06.993776Z","shell.execute_reply.started":"2024-06-10T17:24:37.485391Z","shell.execute_reply":"2024-06-10T17:29:06.992870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Zip the result file**","metadata":{}},{"cell_type":"code","source":"!zip -r ./file.zip ./results ","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:29:12.949689Z","iopub.execute_input":"2024-06-10T17:29:12.950027Z","iopub.status.idle":"2024-06-10T17:29:18.891225Z","shell.execute_reply.started":"2024-06-10T17:29:12.949995Z","shell.execute_reply":"2024-06-10T17:29:18.890180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**download te results file**","metadata":{}},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'file.zip')","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:31:22.544265Z","iopub.execute_input":"2024-06-10T17:31:22.544702Z","iopub.status.idle":"2024-06-10T17:31:22.553163Z","shell.execute_reply.started":"2024-06-10T17:31:22.544667Z","shell.execute_reply":"2024-06-10T17:31:22.552132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Test**","metadata":{}}]}